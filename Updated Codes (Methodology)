import pandas as pd
#Reading file path#
file_path = r"C:\Users\olich\OneDrive\Desktop\Master Thesis\Master Final Data.xlsx"

xls = pd.ExcelFile(file_path)
xls.sheet_names

spxt = pd.read_excel(file_path, sheet_name = "SPXT Index")
spxt.head()

#Clean the data with date set as date
spxt_clean = spxt[["Dates", "PX_LAST"]].copy()
spxt_clean["Dates"] = pd.to_datetime(spxt_clean["Dates"])
spxt_clean = spxt_clean.set_index("Dates")
spxt_clean = spxt_clean.sort_index()
spxt_clean.head()

#Check the timeperiod and fix it from Sept. 1989 - Sept. 2025, 36 years of data
spxt_clean.index.min(),spxt_clean.index.max(),spxt_clean.index.to_series().diff().value_counts().head()

spxt_clean = spxt_clean.loc["1989-09-01":"2025-09-30"]
spxt_clean.index.min(), spxt_clean.index.max()

#Calculate simple monthly return for SPXT
spxt_ret = spxt_clean["PX_LAST"].pct_change().dropna()
spxt_ret.head(), spxt_ret.tail()

#Rename the SPXT series
spxt_ret.name = "SPXT"

#Repeat the same for Bond LUACTRUU
lu = pd.read_excel(file_path, sheet_name = "LUACTRUU Index")
lu.head()

lu_clean = lu[["Dates", "PX_LAST"]].copy()
lu_clean["Dates"] = pd.to_datetime(lu_clean["Dates"])
lu_clean = lu_clean.set_index("Dates")
lu_clean = lu_clean.sort_index()
lu_clean.head()

lu_clean = lu_clean.loc["1989-09-01":"2025-09-30"]
lu_clean.index.min(), lu_clean.index.max()

lu_ret = lu_clean["PX_LAST"].pct_change().dropna()
lu_ret.name = "LUACTRUU"

lu_ret.head()

lu_ret.tail()

#Repeat the same for Bond FDTR
fdtr = pd.read_excel(file_path, sheet_name = "FDTR Index")
fdtr.head()

fdtr_clean = fdtr[["Dates", "PX_LAST"]].copy()
fdtr_clean["Dates"] = pd.to_datetime(fdtr_clean["Dates"])
fdtr_clean = fdtr_clean.set_index("Dates").sort_index()

fdtr_clean = fdtr_clean.loc["1989-09-01":"2025-09-30"].dropna()

fdtr_clean.index.min(), fdtr_clean.index.max(), fdtr_clean.head()
#Convert annualiazed rate into Monthly compounded returns#
fdtr_ret = (1 + fdtr_clean["PX_LAST"] / 100) ** (1/12) - 1
fdtr_ret.name = "RF"
fdtr_ret.head(), fdtr_ret.tail()

#Repeat the same for FNER
fner = pd.read_excel(file_path, sheet_name="FNER Index")
fner.head()

fner_clean = fner[["Dates", "PX_LAST"]].copy()
fner_clean["Dates"] = pd.to_datetime(fner_clean["Dates"])
fner_clean = fner_clean.set_index("Dates").sort_index()

fner_clean.head()

fner_clean = fner_clean.loc["1989-09-01":"2025-09-30"]

fner_clean.index.min(), fner_clean.index.max()

fner_ret = fner_clean["PX_LAST"].pct_change().dropna()

fner_ret.name = "FNER"

fner_ret.head(), fner_ret.tail()

#Repeat the same for SPGCCITR
spgccitr = pd.read_excel(file_path, sheet_name="SPGCCITR Index")
spgccitr.head()

spgccitr_clean = spgccitr[["Dates", "PX_LAST"]].copy()

spgccitr_clean["Dates"] = pd.to_datetime(spgccitr_clean["Dates"])
spgccitr_clean = spgccitr_clean.set_index("Dates").sort_index()

spgccitr_clean.head()

spgccitr_clean = spgccitr_clean.loc["1989-09-01":"2025-09-30"]

spgccitr_clean.index.min(), spgccitr_clean.index.max()

spgccitr_ret = spgccitr_clean["PX_LAST"].pct_change().dropna()

spgccitr_ret.name = "SPGCCITR"

spgccitr_ret.head(), spgccitr_ret.tail()

#Combine all return series into a dataframe
returns_core = pd.concat(
    [spxt_ret, lu_ret, fdtr_ret, fner_ret, spgccitr_ret],
    axis=1
)

#Drop the first missing values
returns_core = returns_core.dropna()

returns_core.head(), returns_core.tail()
returns_core.head(), returns_core.tail()

#Calculate monthly statistics
summary_stats = pd.DataFrame({
    "Mean": returns_core.mean(),
    "Std": returns_core.std(),
    "Sharpe": returns_core.mean() / returns_core.std()
})

#Calculate annual statistics
summary_stats_ann = pd.DataFrame({
    "Mean (ann)": returns_core.mean() * 12,
    "Std (ann)": returns_core.std() * (12 ** 0.5),
    "Sharpe": (returns_core.mean() / returns_core.std()) * (12 ** 0.5)
})

summary_stats_ann
summary_stats

#Calculate correlations of asset classes
corr_matrix = returns_core.corr()
corr_matrix

# Compute Mean returns vector and covariance
import numpy as np
mu = returns_core.mean().values
Sigma = returns_core.cov().values

#Invert covariance matrix and create vectors of 1
Sigma_inv = np.linalg.inv(Sigma)
ones = np.ones(len(mu))
w_gmv = Sigma_inv @ ones / (ones @ Sigma_inv @ ones)

#Compute Global Minimum Variance Portfolio Weights
gmv_weights = pd.Series(w_gmv, index = returns_core.columns)
gmv_weights

from scipy.optimize import minimize
import numpy as np

#Calculating returns with and without commodities
returns_jensen_base = returns_core[["SPXT", "LUACTRUU", "FNER", "RF"]]          
returns_jensen_with = returns_core[["SPXT", "LUACTRUU", "FNER", "RF", "SPGCCITR"]] 

returns_jensen_base.columns, returns_jensen_with.columns

#Define Efficient Frontier with 30 points
def compute_frontier_no_short(returns_df, n_points=30):
    mu = returns_df.mean().values
    Sigma = returns_df.cov().values
    n = len(mu)

#Initial guess and constraint no short selling
    w0 = np.ones(n) / n
    bounds = tuple((0, 1) for _ in range(n))

#define portfolio variance and create target returns
    def var_obj(w):
        return w.T @ Sigma @ w

    target_grid = np.linspace(mu.min(), mu.max(), n_points)

    vol = []
    ret = []

#Include constraints in the loop
    for target in target_grid:
        cons = (
            {"type": "eq", "fun": lambda w: np.sum(w) - 1},
            {"type": "eq", "fun": lambda w, target=target: w @ mu - target},
        )

#Optimize portfolio variance
        res = minimize(var_obj, w0, method="SLSQP", bounds=bounds, constraints=cons)

        if res.success:
            vol.append(np.sqrt(res.fun))
            ret.append(target)

    return np.array(vol), np.array(ret)

#Compute Efficient Frontier with and without commodities
vol_base_m, ret_base_m = compute_frontier_no_short(returns_jensen_base, n_points=30)
vol_with_m, ret_with_m = compute_frontier_no_short(returns_jensen_with, n_points=30)

#Annualize the std. and return
vol_base_ann = vol_base_m * np.sqrt(12)
ret_base_ann = ret_base_m * 12
vol_with_ann = vol_with_m * np.sqrt(12)
ret_with_ann = ret_with_m * 12

#Plot the Efficient Frontier
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 6))

ax.plot(vol_base_ann * 100, ret_base_ann * 100, marker="o", linestyle="-",
        label="Frontier: Stocks, Corp Bonds, REITs, T-bills")
ax.plot(vol_with_ann * 100, ret_with_ann * 100, marker="o", linestyle="-",
        label="Frontier: + Commodity Futures")

ax.set_xlabel("Standard Deviation (%)")
ax.set_ylabel("Expected Return (%)")
ax.set_title("Efficient Frontiers (No Short Selling, Annualized)")
ax.legend()
ax.grid(True)

fig.savefig("efficient_frontier_comparison.png", dpi=300, bbox_inches="tight")
plt.show()


# Monetary Policy Introduction
# Align Policy Rate FEDL01 Index to the asset classes and clean it
fed = pd.read_excel(file_path, sheet_name="FEDL01 Index")

fed_clean = fed[["Dates", "PX_LAST"]].copy()

fed_clean["Dates"] = pd.to_datetime(fed_clean["Dates"])
fed_clean = fed_clean.set_index("Dates").sort_index()

fed_clean = fed_clean.loc["1989-09-01":"2025-09-30"].dropna()

fed_clean.index.min(), fed_clean.index.max(), fed_clean.head()

#Finding the policy rate changes with difference from previous period
fed_clean["d_rate"] = fed_clean["PX_LAST"].diff()

#Creating binary indicator for Expansionary and Restrictive Monetary Policy
fed_clean["restrictive"] = (fed_clean["d_rate"] > 0).astype(int)

fed_clean[["PX_LAST", "d_rate", "restrictive"]].head(10)

#Define Regime with Binary Variable
regime = fed_clean[["restrictive"]]

#Convert returns to monthly figures for alignment with Regime
returns_regime_m = returns_core.copy()
returns_regime_m.index = returns_regime_m.index.to_period("M")

fed_regime_m = fed_clean.copy()
fed_regime_m.index = fed_regime_m.index.to_period("M")

#Merge regime with return of asset classes
returns_regime_m = returns_regime_m.merge(
    fed_regime_m[["restrictive"]],
    left_index=True,
    right_index=True,
    how="inner"
)

returns_regime_m.head(), returns_regime_m.shape

#Segregate returns data into Expansionary and Restrictive periods
returns_exp = returns_regime_m[returns_regime_m["restrictive"] == 0].drop(columns=["restrictive"])
returns_res = returns_regime_m[returns_regime_m["restrictive"] == 1].drop(columns=["restrictive"])

returns_exp.shape, returns_res.shape

#Compute statistics for expansionary periods
mean_exp = returns_exp.mean()
std_exp  = returns_exp.std()

stats_exp = pd.DataFrame({
    "Mean": mean_exp,
    "Std": std_exp,
    "CoV": std_exp / mean_exp,                 
    "Sharpe": mean_exp / std_exp               
})

#Compute statistics for restrictive periods
mean_res = returns_res.mean()
std_res  = returns_res.std()

stats_res = pd.DataFrame({
    "Mean": mean_res,
    "Std": std_res,
    "CoV": std_res / mean_res,
    "Sharpe": mean_res / std_res
})

#Multiply Mean and Std by 100 to produce the same table - 2 as the paper
stats_exp_pct = stats_exp.copy()
stats_res_pct = stats_res.copy()

stats_exp_pct[["Mean", "Std"]] = stats_exp_pct[["Mean", "Std"]] * 100
stats_res_pct[["Mean", "Std"]] = stats_res_pct[["Mean", "Std"]] * 100

stats_exp_pct, stats_res_pct

#Calculate correlations for the different periods
corr_exp = returns_exp.corr()
corr_res = returns_res.corr()
corr_exp, corr_res

# Test difference for Returns and Risk across Monetary Environments
from scipy import stats
import numpy as np
import pandas as pd

assets = ["SPXT", "LUACTRUU", "RF", "FNER", "SPGCCITR"]

rows = []
for a in assets:
    x = returns_exp[a].dropna()
    y = returns_res[a].dropna()

    # Difference in mean monthly returns (%)
    diff_mean_pct = (y.mean() - x.mean()) * 100

    # Welch t-test for difference in means 
    t_stat, p_val = stats.ttest_ind(y, x, equal_var=False, nan_policy="omit")

    # F-test for difference in variances
    f_stat = (y.var(ddof=1) / x.var(ddof=1))

    rows.append([diff_mean_pct, t_stat, p_val, f_stat])

Test_diff = pd.DataFrame(
    rows,
    index=assets,
    columns=["Diff Mean (%) (Res-Exp)", "t-stat", "p-value", "F-stat (Var Res / Var Exp)"]
)

Test_diff

#Check for T-test and F-test for Return and Std. for Table representation
Test_diff_fmt = Test_diff.copy()

Test_diff_fmt["Diff Mean (%) (Res-Exp)"] = Test_diff_fmt.apply(
    lambda r: f"{r['Diff Mean (%) (Res-Exp)']:.4f}" + ("*" if r["p-value"] < 0.05 else ""),
    axis=1
)

Test_diff_fmt["t-stat"] = Test_diff_fmt["t-stat"].map(lambda x: f"{x:.2f}")
Test_diff_fmt["F-stat (Var Res / Var Exp)"] = Test_diff_fmt["F-stat (Var Res / Var Exp)"].map(lambda x: f"{x:.3f}")

Test_diff_fmt

#Compute efficient frontiers with expansionary and restrictive periods with and without commodities
exp_base  = returns_exp[["SPXT", "LUACTRUU", "FNER", "RF"]]
exp_with  = returns_exp[["SPXT", "LUACTRUU", "FNER", "RF", "SPGCCITR"]]

res_base  = returns_res[["SPXT", "LUACTRUU", "FNER", "RF"]]
res_with  = returns_res[["SPXT", "LUACTRUU", "FNER", "RF", "SPGCCITR"]]

vol_exp_base_m, ret_exp_base_m = compute_frontier_no_short(exp_base, n_points=30)
vol_exp_with_m, ret_exp_with_m = compute_frontier_no_short(exp_with, n_points=30)

vol_res_base_m, ret_res_base_m = compute_frontier_no_short(res_base, n_points=30)
vol_res_with_m, ret_res_with_m = compute_frontier_no_short(res_with, n_points=30)

import numpy as np
import matplotlib.pyplot as plt

def annualize_frontier(vol_m, ret_m):
    vol_a = vol_m * np.sqrt(12)
    ret_a = ret_m * 12
    return vol_a, ret_a

vol_exp_base_a, ret_exp_base_a = annualize_frontier(vol_exp_base_m, ret_exp_base_m)
vol_exp_with_a, ret_exp_with_a = annualize_frontier(vol_exp_with_m, ret_exp_with_m)

vol_res_base_a, ret_res_base_a = annualize_frontier(vol_res_base_m, ret_res_base_m)
vol_res_with_a, ret_res_with_a = annualize_frontier(vol_res_with_m, ret_res_with_m)

fig1, ax1 = plt.subplots(figsize=(8, 6))
ax1.plot(vol_exp_base_a*100, ret_exp_base_a*100, marker="o", label="No Futures")
ax1.plot(vol_exp_with_a*100, ret_exp_with_a*100, marker="o", label="+ Futures")
ax1.set_xlabel("Standard Deviation (%)")
ax1.set_ylabel("Expected Return (%)")
ax1.set_title("Efficient Frontiers – Expansionary Periods (Annualized)")
ax1.legend()
ax1.grid(True)
fig1.savefig("frontier_expansionary.png", dpi=300, bbox_inches="tight")
plt.show()

fig2, ax2 = plt.subplots(figsize=(8, 6))
ax2.plot(vol_res_base_a*100, ret_res_base_a*100, marker="o", label="No Futures")
ax2.plot(vol_res_with_a*100, ret_res_with_a*100, marker="o", label="+ Futures")
ax2.set_xlabel("Standard Deviation (%)")
ax2.set_ylabel("Expected Return (%)")
ax2.set_title("Efficient Frontiers – Restrictive Periods (Annualized)")
ax2.legend()
ax2.grid(True)
fig2.savefig("frontier_restrictive.png", dpi=300, bbox_inches="tight")
plt.show()

#Calculate Optimal Weights of Portfolio without differentiating the monetary periods
import numpy as np
import pandas as pd
from scipy.optimize import minimize

R = returns_jensen_with.copy()   
mu = R.mean().values
Sigma = R.cov().values
assets = R.columns.tolist()
n = len(assets)

def max_return_given_vol(target_vol):
    # maximize w@mu subject to: sum(w)=1, w>=0, sqrt(w'Σw)=target_vol
    w0 = np.ones(n) / n
    bounds = [(0, 1)] * n

    def obj(w):
        return -(w @ mu)  # maximize return

    cons = [
        {"type": "eq", "fun": lambda w: np.sum(w) - 1},
        {"type": "eq", "fun": lambda w, tv=target_vol: (w.T @ Sigma @ w) - tv**2},
    ]

    res = minimize(obj, w0, method="SLSQP", bounds=bounds, constraints=cons)
    return res


# Keeping the target Std from 0.5 to 4.0
target_vols_pct = np.arange(0.5, 4.0 + 0.5, 0.5)     # 0.5% to 4.0%
target_vols = target_vols_pct / 100                  # convert to decimals

rows = []
for tv_pct, tv in zip(target_vols_pct, target_vols):
    res = max_return_given_vol(tv)

    if res.success:
        w = res.x
        port_ret = w @ mu
        rows.append([tv_pct, port_ret * 100] + list(w * 100))  # return in %, weights in %
    else:
        rows.append([tv_pct, np.nan] + [np.nan] * n)

PW = pd.DataFrame(rows, columns=["Portfolio Std Dev (%)", "Expected Return (%)"] + assets)
PW


# Check whether the portfolio weights sum to 1
PW["Weight Sum (%)"] = PW[assets].sum(axis=1)
min_w = PW[assets].min().min()
max_w = PW[assets].max().max()

#Rounding the wieghts to 2 decimal places
PW_clean = PW.copy()
num_cols = PW_clean.columns  # all are numeric here

PW_clean[num_cols] = PW_clean[num_cols].applymap(lambda x: 0 if abs(x) < 1e-10 else x)

PW_round = PW_clean.round(2)

# Comparing the same without any commodity futures portfolio
R_nf = returns_jensen_base.copy()   # SPXT, LUACTRUU, FNER, RF
mu_nf = R_nf.mean().values
Sigma_nf = R_nf.cov().values
assets_nf = R_nf.columns.tolist()
n_nf = len(assets_nf)

def max_return_given_vol_nf(target_vol):
    w0 = np.ones(n_nf) / n_nf
    bounds = [(0, 1)] * n_nf

    def obj(w):
        return -(w @ mu_nf)

    cons = [
        {"type": "eq", "fun": lambda w: np.sum(w) - 1},
        {"type": "eq", "fun": lambda w, tv=target_vol: (w.T @ Sigma_nf @ w) - tv**2},
    ]

    return minimize(obj, w0, method="SLSQP", bounds=bounds, constraints=cons)

rows_nf = []
for tv_pct, tv in zip(target_vols_pct, target_vols):
    res = max_return_given_vol_nf(tv)

    if res.success:
        w = res.x
        port_ret = w @ mu_nf
        rows_nf.append([tv_pct, port_ret * 100] + list(w * 100))
    else:
        rows_nf.append([tv_pct, np.nan] + [np.nan] * n_nf)

PW_nf = pd.DataFrame(rows_nf, columns=["Portfolio Std Dev (%)", "Expected Return (%)"] + assets_nf)

# Clean numbers and round up
PW_nf = PW_nf.applymap(lambda x: 0 if pd.notnull(x) and abs(x) < 1e-10 else x).round(2)
PW_nf["Weight Sum (%)"] = PW_nf[assets_nf].sum(axis=1)

PW_nf

PW_round

PW[["Portfolio Std Dev (%)", "Expected Return (%)", "Weight Sum (%)"]], (min_w, max_w)


#Compute the incremental Commodity Futures weight
#Align portfolio by Std
A = PW_round.set_index("Portfolio Std Dev (%)")
B = PW_nf.set_index("Portfolio Std Dev (%)")

#Keeping weights
w_with = A[["SPXT","LUACTRUU","FNER","RF","SPGCCITR"]].copy()
w_nof  = B[["SPXT","LUACTRUU","FNER","RF"]].copy()

#compare portfolio with and without commodity futures 
delta_common = w_with[["SPXT","LUACTRUU","FNER","RF"]] - w_nof

#Build a comparison table
comp = pd.concat(
    [
        w_nof.add_prefix("NoFut_"),
        w_with.add_prefix("WithFut_"),
        delta_common.add_prefix("Delta_"),
        w_with[["SPGCCITR"]].rename(columns={"SPGCCITR":"Futures_Weight"})
    ],
    axis=1
).round(2)

comp
