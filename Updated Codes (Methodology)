import pandas as pd
#Reading file path#
file_path = r"C:\Users\olich\OneDrive\Desktop\Master Thesis\Master Final Data.xlsx"

xls = pd.ExcelFile(file_path)
xls.sheet_names

spxt = pd.read_excel(file_path, sheet_name = "SPXT Index")
spxt.head()

#Clean the data with date set as date
spxt_clean = spxt[["Dates", "PX_LAST"]].copy()
spxt_clean["Dates"] = pd.to_datetime(spxt_clean["Dates"])
spxt_clean = spxt_clean.set_index("Dates")
spxt_clean = spxt_clean.sort_index()
spxt_clean.head()

#Check the timeperiod and fix it from Sept. 1989 - Sept. 2025, 36 years of data
spxt_clean.index.min(),spxt_clean.index.max(),spxt_clean.index.to_series().diff().value_counts().head()

spxt_clean = spxt_clean.loc["1989-09-01":"2025-09-30"]
spxt_clean.index.min(), spxt_clean.index.max()

#Calculate simple monthly return for SPXT
spxt_ret = spxt_clean["PX_LAST"].pct_change().dropna()
spxt_ret.head(), spxt_ret.tail()

#Rename the SPXT series
spxt_ret.name = "SPXT"

#Repeat the same for Bond LUACTRUU
lu = pd.read_excel(file_path, sheet_name = "LUACTRUU Index")
lu.head()

lu_clean = lu[["Dates", "PX_LAST"]].copy()
lu_clean["Dates"] = pd.to_datetime(lu_clean["Dates"])
lu_clean = lu_clean.set_index("Dates")
lu_clean = lu_clean.sort_index()
lu_clean.head()

lu_clean = lu_clean.loc["1989-09-01":"2025-09-30"]
lu_clean.index.min(), lu_clean.index.max()

lu_ret = lu_clean["PX_LAST"].pct_change().dropna()
lu_ret.name = "LUACTRUU"

lu_ret.head()

lu_ret.tail()

#Repeat the same for Bond FDTR
fdtr = pd.read_excel(file_path, sheet_name = "FDTR Index")
fdtr.head()

fdtr_clean = fdtr[["Dates", "PX_LAST"]].copy()
fdtr_clean["Dates"] = pd.to_datetime(fdtr_clean["Dates"])
fdtr_clean = fdtr_clean.set_index("Dates").sort_index()

fdtr_clean = fdtr_clean.loc["1989-09-01":"2025-09-30"].dropna()

fdtr_clean.index.min(), fdtr_clean.index.max(), fdtr_clean.head()
#Convert annualiazed rate into Monthly compounded returns#
fdtr_ret = (1 + fdtr_clean["PX_LAST"] / 100) ** (1/12) - 1
fdtr_ret.name = "RF"
fdtr_ret.head(), fdtr_ret.tail()

#Repeat the same for FNER
fner = pd.read_excel(file_path, sheet_name="FNER Index")
fner.head()

fner_clean = fner[["Dates", "PX_LAST"]].copy()
fner_clean["Dates"] = pd.to_datetime(fner_clean["Dates"])
fner_clean = fner_clean.set_index("Dates").sort_index()

fner_clean.head()

fner_clean = fner_clean.loc["1989-09-01":"2025-09-30"]

fner_clean.index.min(), fner_clean.index.max()

fner_ret = fner_clean["PX_LAST"].pct_change().dropna()

fner_ret.name = "FNER"

fner_ret.head(), fner_ret.tail()

#Repeat the same for SPGCCITR
spgccitr = pd.read_excel(file_path, sheet_name="SPGCCITR Index")
spgccitr.head()

spgccitr_clean = spgccitr[["Dates", "PX_LAST"]].copy()

spgccitr_clean["Dates"] = pd.to_datetime(spgccitr_clean["Dates"])
spgccitr_clean = spgccitr_clean.set_index("Dates").sort_index()

spgccitr_clean.head()

spgccitr_clean = spgccitr_clean.loc["1989-09-01":"2025-09-30"]

spgccitr_clean.index.min(), spgccitr_clean.index.max()

spgccitr_ret = spgccitr_clean["PX_LAST"].pct_change().dropna()

spgccitr_ret.name = "SPGCCITR"

spgccitr_ret.head(), spgccitr_ret.tail()

#Combine all return series into a dataframe
returns_core = pd.concat(
    [spxt_ret, lu_ret, fdtr_ret, fner_ret, spgccitr_ret],
    axis=1
)

#Drop the first missing values
returns_core = returns_core.dropna()

returns_core.head(), returns_core.tail()
returns_core.head(), returns_core.tail()

#Calculate monthly statistics
summary_stats = pd.DataFrame({
    "Mean": returns_core.mean(),
    "Std": returns_core.std(),
    "Sharpe": returns_core.mean() / returns_core.std()
})

#Calculate annual statistics
summary_stats_ann = pd.DataFrame({
    "Mean (ann)": returns_core.mean() * 12,
    "Std (ann)": returns_core.std() * (12 ** 0.5),
    "Sharpe": (returns_core.mean() / returns_core.std()) * (12 ** 0.5)
})

summary_stats_ann
summary_stats

#Calculate correlations of asset classes
corr_matrix = returns_core.corr()
corr_matrix

# Compute Mean returns vector and covariance
import numpy as np
mu = returns_core.mean().values
Sigma = returns_core.cov().values

#Invert covariance matrix and create vectors of 1
Sigma_inv = np.linalg.inv(Sigma)
ones = np.ones(len(mu))
w_gmv = Sigma_inv @ ones / (ones @ Sigma_inv @ ones)

#Compute Global Minimum Variance Portfolio Weights
gmv_weights = pd.Series(w_gmv, index = returns_core.columns)
gmv_weights

from scipy.optimize import minimize
import numpy as np

#Calculating returns with and without commodities
returns_jensen_base = returns_core[["SPXT", "LUACTRUU", "FNER", "RF"]]          
returns_jensen_with = returns_core[["SPXT", "LUACTRUU", "FNER", "RF", "SPGCCITR"]] 

returns_jensen_base.columns, returns_jensen_with.columns

#Define Efficient Frontier with 30 points
def compute_frontier_no_short(returns_df, n_points=30):
    mu = returns_df.mean().values
    Sigma = returns_df.cov().values
    n = len(mu)

#Initial guess and constraint no short selling
    w0 = np.ones(n) / n
    bounds = tuple((0, 1) for _ in range(n))

#define portfolio variance and create target returns
    def var_obj(w):
        return w.T @ Sigma @ w

    target_grid = np.linspace(mu.min(), mu.max(), n_points)

    vol = []
    ret = []

#Include constraints in the loop
    for target in target_grid:
        cons = (
            {"type": "eq", "fun": lambda w: np.sum(w) - 1},
            {"type": "eq", "fun": lambda w, target=target: w @ mu - target},
        )

#Optimize portfolio variance
        res = minimize(var_obj, w0, method="SLSQP", bounds=bounds, constraints=cons)

        if res.success:
            vol.append(np.sqrt(res.fun))
            ret.append(target)

    return np.array(vol), np.array(ret)

#Compute Efficient Frontier with and without commodities
vol_base_m, ret_base_m = compute_frontier_no_short(returns_jensen_base, n_points=30)
vol_with_m, ret_with_m = compute_frontier_no_short(returns_jensen_with, n_points=30)

#Annualize the std. and return
vol_base_ann = vol_base_m * np.sqrt(12)
ret_base_ann = ret_base_m * 12
vol_with_ann = vol_with_m * np.sqrt(12)
ret_with_ann = ret_with_m * 12

#Plot the Efficient Frontier
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(8, 6))

ax.plot(vol_base_ann * 100, ret_base_ann * 100, marker="o", linestyle="-",
        label="Frontier: Stocks, Corp Bonds, REITs, T-bills")
ax.plot(vol_with_ann * 100, ret_with_ann * 100, marker="o", linestyle="-",
        label="Frontier: + Commodity Futures")

ax.set_xlabel("Standard Deviation (%)")
ax.set_ylabel("Expected Return (%)")
ax.set_title("Efficient Frontiers (No Short Selling, Annualized)")
ax.legend()
ax.grid(True)

fig.savefig("efficient_frontier_comparison.png", dpi=300, bbox_inches="tight")
plt.show()


# Monetary Policy Introduction
# Align Policy Rate FEDL01 Index to the asset classes and clean it
fed = pd.read_excel(file_path, sheet_name="FEDL01 Index")

fed_clean = fed[["Dates", "PX_LAST"]].copy()

fed_clean["Dates"] = pd.to_datetime(fed_clean["Dates"])
fed_clean = fed_clean.set_index("Dates").sort_index()

fed_clean = fed_clean.loc["1989-09-01":"2025-09-30"].dropna()

fed_clean.index.min(), fed_clean.index.max(), fed_clean.head()

#Finding the policy rate changes with difference from previous period
fed_clean["d_rate"] = fed_clean["PX_LAST"].diff()

#Creating binary indicator for Expansionary and Restrictive Monetary Policy
fed_clean["restrictive"] = (fed_clean["d_rate"] > 0).astype(int)

fed_clean[["PX_LAST", "d_rate", "restrictive"]].head(10)

#Define Regime with Binary Variable
regime = fed_clean[["restrictive"]]

#Convert returns to monthly figures for alignment with Regime
returns_regime_m = returns_core.copy()
returns_regime_m.index = returns_regime_m.index.to_period("M")

fed_regime_m = fed_clean.copy()
fed_regime_m.index = fed_regime_m.index.to_period("M")

#Merge regime with return of asset classes
returns_regime_m = returns_regime_m.merge(
    fed_regime_m[["restrictive"]],
    left_index=True,
    right_index=True,
    how="inner"
)

returns_regime_m.head(), returns_regime_m.shape

#Segregate returns data into Expansionary and Restrictive periods
returns_exp = returns_regime_m[returns_regime_m["restrictive"] == 0].drop(columns=["restrictive"])
returns_res = returns_regime_m[returns_regime_m["restrictive"] == 1].drop(columns=["restrictive"])

returns_exp.shape, returns_res.shape

